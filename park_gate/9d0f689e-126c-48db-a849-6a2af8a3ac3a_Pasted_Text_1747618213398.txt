import logging
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional
import numpy as np
import cv2
from scipy.spatial.transform import Rotation as R

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

@dataclass
class Point3DView:
    coords: np.ndarray
    rgb: Optional[Tuple[int, int, int]] = None  # RGB color of the point
    observations: Dict[int, int] = field(default_factory=dict)

@dataclass
class ReconstructionCfg:
    K: np.ndarray
    min_inliers_baseline: int = 10
    essential_ransac_thresh: float = 2.0
    pnp_reproj_thresh: float = 4.0
    pnp_iterations: int = 1000
    min_pnp_correspondences: int = 10
    max_failed_attempts: int = 3
    bundle_every: int = 3
    verbose: bool = True

class Reconstruction:
    def __init__(
        self,
        keypoints: List[List[cv2.KeyPoint]],
        matches: Dict[Tuple[int, int], List[cv2.DMatch]],
        img_adjacency: np.ndarray,
        cfg: ReconstructionCfg,
        images: List[np.ndarray]  # <-- Moved after cfg
    ):
        self.keypoints = keypoints
        self.matches = matches
        self.adjacency = img_adjacency
        self.cfg = cfg
        self.images = images  # Now we can use it in triangulation
        self.poses: Dict[int, Tuple[np.ndarray, np.ndarray]] = {}
        self.points3d: List[Point3DView] = []
        self.placed: List[int] = []
        self.unplaced: List[int] = list(range(img_adjacency.shape[0]))
        self.failed_attempts: Dict[int, int] = {}
        self.add_count = 0

    def select_baseline(self, top_percent: float = 0.3) -> Tuple[int, int]:
        scores = []
        for (i, j), mlist in self.matches.items():
            if len(mlist) < self.cfg.min_inliers_baseline:
                continue
            pts_i, pts_j = self._aligned_points(i, j)
            E, mask = cv2.findEssentialMat(
                pts_i, pts_j, self.cfg.K,
                method=cv2.FM_RANSAC,
                threshold=self.cfg.essential_ransac_thresh
            )
            if mask is None or mask.sum() < self.cfg.min_inliers_baseline:
                continue
            _, R, t, out_mask = cv2.recoverPose(E, pts_i, pts_j, self.cfg.K)
            scores.append(((i, j), len(mlist), int(out_mask.sum()), np.linalg.norm(t)))
        
        scores.sort(key=lambda x: x[2], reverse=True)
        best = scores[0]
        logger.info(f"Baseline pair: {best[0]} with {best[1]} matches, {best[2]} inliers")
        return best[0]

    def initialize(self, baseline: Tuple[int, int]) -> None:
        i, j = baseline
        pts_i, pts_j, idxs_i, idxs_j = self._aligned_points(i, j, return_idxs=True)

        E, mask = cv2.findEssentialMat(
            pts_i, pts_j, self.cfg.K,
            method=cv2.FM_RANSAC,
            threshold=self.cfg.essential_ransac_thresh
        )

        if mask is None or mask.sum() < 15:
            raise ValueError("Baseline failed due to insufficient inliers")

        _, R, t, pose_mask = cv2.recoverPose(E, pts_i, pts_j, self.cfg.K)

        inliers = pose_mask.ravel().astype(bool)
        if inliers.sum() < 15:
            raise ValueError("Pose recovery failed with low inliers")

        self.poses[i] = (np.eye(3), np.zeros((3, 1)))
        self.poses[j] = (R, t)

        idxs_i = np.array(idxs_i)[inliers]
        idxs_j = np.array(idxs_j)[inliers]

        self._triangulate_and_add(i, j, idxs_i, idxs_j)

        self.placed = [i, j]
        self.unplaced.remove(i)
        self.unplaced.remove(j)
        logger.info("Initialized reconstruction with baseline image pair.")

    def grow(self, bundle_adjust_fn=None):
        while self.unplaced:
            valid_unplaced = [
                u for u in self.unplaced
                if self.failed_attempts.get(u, 0) < self.cfg.max_failed_attempts
            ]
            if not valid_unplaced:
                logger.warning("All unplaced images have exceeded failure limit.")
                break

            try:
                best_img, best_corr = max(
                    ((u, self._count_correspondences(u)) for u in valid_unplaced),
                    key=lambda x: x[1]
                )
            except ValueError:
                logger.warning("No valid images with correspondences.")
                break

            if best_corr < self.cfg.min_pnp_correspondences:
                logger.warning(f"Image {best_img} has only {best_corr} correspondence(s); skipping.")
                self.unplaced.remove(best_img)
                continue

            try:
                self._add_image(best_img)
                self.add_count += 1
                if bundle_adjust_fn and self.add_count % self.cfg.bundle_every == 0:
                    bundle_adjust_fn()
                if best_img in self.failed_attempts:
                    del self.failed_attempts[best_img]
            except Exception as e:
                logger.error(f"Failed to add image {best_img}: {str(e)}")
                self.failed_attempts[best_img] = self.failed_attempts.get(best_img, 0) + 1
                if self.failed_attempts[best_img] >= self.cfg.max_failed_attempts:
                    logger.warning(f"Permanently removing image {best_img}")
                    self.unplaced.remove(best_img)

    def _count_correspondences(self, img_idx: int) -> int:
        count = 0
        for pt in self.points3d:
            for p in self.placed:
                pair = tuple(sorted((p, img_idx)))
                for m in self.matches.get(pair, []):
                    q, t = (m.queryIdx, m.trainIdx) if p < img_idx else (m.trainIdx, m.queryIdx)
                    if (
                        q < len(self.keypoints[p]) and
                        t < len(self.keypoints[img_idx]) and
                        pt.observations.get(p) == q
                    ):
                        count += 1
                        break
        return count

    def _add_image(self, img_idx: int) -> None:
        pts3d, pts2d, pt_objs = [], [], []

        # Collect existing 3D-2D matches from placed images
        for pt3d in self.points3d:
            for p in self.placed:
                pair = tuple(sorted((p, img_idx)))
                for m in self.matches.get(pair, []):
                    q, t = (m.queryIdx, m.trainIdx) if p < img_idx else (m.trainIdx, m.queryIdx)
                    if (
                        q < len(self.keypoints[p]) and
                        t < len(self.keypoints[img_idx]) and
                        pt3d.observations.get(p) == q
                    ):
                        pts3d.append(pt3d.coords)
                        pts2d.append(self.keypoints[img_idx][t].pt)
                        pt_objs.append((pt3d, t))
                        break

        # If not enough, fall back to triangulate new matches with best neighbor
        if len(pts3d) < self.cfg.min_pnp_correspondences:
            candidates = sorted(
                [(p, len(self.matches.get(tuple(sorted((p, img_idx))), []))) for p in self.placed],
                key=lambda x: x[1], reverse=True
            )
            if not candidates:
                raise ValueError("No placed image has matches with this one")

            best_p, _ = candidates[0]
            R_bp, t_bp = self.poses[best_p]

            # Skip identity pose unless initial stage
            if np.linalg.norm(t_bp) < 1e-6 and len(self.placed) > 2:
                raise ValueError("Neighbor has invalid identity pose")

            raw_matches = self.matches.get(tuple(sorted((best_p, img_idx))), [])
            match_list = [
                m for m in raw_matches
                if m.queryIdx < len(self.keypoints[best_p]) and
                   m.trainIdx < len(self.keypoints[img_idx])
            ]

            if len(match_list) >= 8:
                idxs_p = [m.queryIdx if best_p < img_idx else m.trainIdx for m in match_list]
                idxs_n = [m.trainIdx if best_p < img_idx else m.queryIdx for m in match_list]

                self._triangulate_and_add(best_p, img_idx, idxs_p, idxs_n)

                # Use small translation in the direction of camera axis
                R_rel = np.eye(3)
                t_rel = np.array([[0.1], [0], [0]])
                R_abs = R_rel @ R_bp
                t_abs = t_bp + R_bp @ t_rel

                self.poses[img_idx] = (R_abs, t_abs)
                self.placed.append(img_idx)
                self.unplaced.remove(img_idx)
                logger.info(f"Added {img_idx} via fallback with {len(match_list)} matches")
                return

        # Run PnP with EPnP or P3P
        if len(pts3d) >= self.cfg.min_pnp_correspondences:
            success, rvec, tvec, inliers = cv2.solvePnPRansac(
                np.array(pts3d).reshape(-1, 3),
                np.array(pts2d).reshape(-1, 2),
                self.cfg.K,
                distCoeffs=None,
                iterationsCount=self.cfg.pnp_iterations,
                reprojectionError=self.cfg.pnp_reproj_thresh,
                flags=cv2.SOLVEPNP_P3P
            )

            if success and inliers is not None and len(inliers) >= 4:
                R, _ = cv2.Rodrigues(rvec)
                self.poses[img_idx] = (R, tvec.reshape(3, 1))

                for idx in inliers.ravel():
                    pt_objs[idx][0].observations[img_idx] = pt_objs[idx][1]

                self.placed.append(img_idx)
                self.unplaced.remove(img_idx)
                logger.info(f"Added image {img_idx} with {len(inliers)} PnP inliers")
                self._triangulate_new_matches(img_idx)
            else:
                logger.warning(f"PnP failed for image {img_idx}")
                raise RuntimeError("PnP failed or insufficient inliers")
        else:
            raise ValueError("Insufficient correspondences for PnP")

    def _aligned_points(self, i: int, j: int, return_idxs: bool = False):
        mlist = self.matches.get(tuple(sorted((i, j))), [])
        pts_i = np.array([self.keypoints[i][m.queryIdx].pt for m in mlist], dtype=np.float32)
        pts_j = np.array([self.keypoints[j][m.trainIdx].pt for m in mlist], dtype=np.float32)

        if return_idxs:
            idxs_i = np.array([m.queryIdx for m in mlist])
            idxs_j = np.array([m.trainIdx for m in mlist])
            return pts_i, pts_j, idxs_i, idxs_j
        return pts_i, pts_j

    def _triangulate_new_matches(self, img_idx: int):
        logger.debug(f"Attempting to triangulate new points for image {img_idx}")
        R_i, t_i = self.poses[img_idx]
        P_i = self.cfg.K @ np.hstack((R_i, t_i))
        new_points = []

        img_n = self.images[img_idx]  # Current image for RGB extraction

        for p in self.placed:
            if p == img_idx:
                continue
            R_j, t_j = self.poses[p]
            P_j = self.cfg.K @ np.hstack((R_j, t_j))
            pair = tuple(sorted((p, img_idx)))
            matches = self.matches.get(pair, [])

            idxs_p, idxs_n = [], []
            for m in matches:
                q, t = (m.queryIdx, m.trainIdx) if p < img_idx else (m.trainIdx, m.queryIdx)
                if q < len(self.keypoints[p]) and t < len(self.keypoints[img_idx]):
                    has_observation = False
                    for pt3d in self.points3d:
                        if pt3d.observations.get(p) == q or pt3d.observations.get(img_idx) == t:
                            has_observation = True
                            break
                    if not has_observation:
                        idxs_p.append(q)
                        idxs_n.append(t)

            if len(idxs_p) < 4:
                continue

            pts_p = np.array([self.keypoints[p][i].pt for i in idxs_p], dtype=np.float64).T.reshape(2, -1)
            pts_n = np.array([self.keypoints[img_idx][i].pt for i in idxs_n], dtype=np.float64).T.reshape(2, -1)

            pts4d = cv2.triangulatePoints(P_j, P_i, pts_p, pts_n)
            pts3d = cv2.convertPointsFromHomogeneous(pts4d.T).squeeze()
            depths = pts3d[:, 2]
            valid = depths > 0.1

            if valid.sum() < 2:
                continue

            img_p = self.images[p]  # Reference image for RGB extraction

            for k in range(len(valid)):
                if not valid[k]:
                    continue
                coords = pts3d[k]
                x_p, y_p = map(int, self.keypoints[p][idxs_p[k]].pt)
                x_n, y_n = map(int, self.keypoints[img_idx][idxs_n[k]].pt)

                # Extract RGB values (convert BGR to RGB)
                rgb_p = tuple(reversed(img_p[y_p, x_p]))
                rgb_n = tuple(reversed(img_n[y_n, x_n]))

                # Average RGB from both views
                avg_rgb = tuple((np.array(rgb_p) + np.array(rgb_n)) // 2)

                pt = Point3DView(
                    coords=coords,
                    rgb=avg_rgb,
                    observations={p: idxs_p[k], img_idx: idxs_n[k]}
                )
                self.points3d.append(pt)
                new_points.append(coords)

            logger.debug(f"Triangulated {valid.sum()} new 3D points between {p} and {img_idx}")

        if new_points:
            logger.info(f"Added {len(new_points)} new colored 3D points during addition of image {img_idx}")
        else:
            logger.warning(f"No new 3D points could be triangulated for image {img_idx}")

    def _triangulate_and_add(self, i: int, j: int, idxs_i: List[int], idxs_j: List[int]) -> None:
        logger.info(f"Triangulating {len(idxs_i)} points between images {i} and {j}")
        
        R_i, t_i = self.poses[i]
        R_j, t_j = self.poses[j]
        P_i = self.cfg.K @ np.hstack((R_i, t_i))
        P_j = self.cfg.K @ np.hstack((R_j, t_j))

        pts_i = np.array([self.keypoints[i][idx].pt for idx in idxs_i], dtype=np.float64).T.reshape(2, -1)
        pts_j = np.array([self.keypoints[j][idx].pt for idx in idxs_j], dtype=np.float64).T.reshape(2, -1)

        pts4d = cv2.triangulatePoints(P_i.astype(np.float64), P_j.astype(np.float64), pts_i, pts_j)
        pts3d = cv2.convertPointsFromHomogeneous(pts4d.T).squeeze()
        depths = pts3d[:, 2]
        valid_mask = depths > 0.1

        if valid_mask.sum() < 2:
            raise ValueError("Not enough valid 3D points after triangulation")

        # Extract RGB values from images
        img_i = self.images[i]
        img_j = self.images[j]

        for k in range(len(valid_mask)):
            if not valid_mask[k]:
                continue
            coords = pts3d[k]
            x_i, y_i = map(int, self.keypoints[i][idxs_i[k]].pt)
            x_j, y_j = map(int, self.keypoints[j][idxs_j[k]].pt)

            # Extract RGB values (convert BGR to RGB)
            rgb_i = tuple(reversed(img_i[y_i, x_i]))  # Convert BGR to RGB
            rgb_j = tuple(reversed(img_j[y_j, x_j]))  # Convert BGR to RGB

            # Average RGB from both views
            avg_rgb = tuple((np.array(rgb_i) + np.array(rgb_j)) // 2)

            pt = Point3DView(
                coords=coords,
                rgb=avg_rgb,  # Store RGB values
                observations={i: idxs_i[k], j: idxs_j[k]}
            )
            self.points3d.append(pt)

        logger.info(f"Successfully added {valid_mask.sum()} new colored 3D points between images {i} and {j}")